{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'TensorRT可视化方法'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"TensorRT可视化方法\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 01 Verbose\n",
    "我们知道，对于onnx、torch、tensorflow来说，模型可以用netron进行可视化，结果很直观，且平台自带的可视化也可以输出layer内容\n",
    "在使用trtexec时，可以使用verbose实现可视化结果，命令行如下指令：\n",
    "`trtexec --explicitBatch --onnx=model/classify.onnx --saveEngine=model/classify_fp32.trt --dumpProfile --exportProfile=debug_profile --verbose`\n",
    "可以得到一些输出，如下：\n",
    "`[01/30/2023-11:52:03] [V] [TRT] Engine Layer Information:\n",
    "Layer(CaskConvolution): Conv_0 + Relu_1, Tactic: 0x27728c886a448c5a, input (Float[1,3,256,256]) -> 633 (Float[1,64,128,128])\n",
    "Layer(CaskPooling): MaxPool_2, Tactic: 0x2639d3932b27ac67, 633 (Float[1,64,128,128]) -> 634 (Float[1,64,64,64])\n",
    "Layer(CaskConvolution): Conv_3 + Relu_4, Tactic: 0x5e7d1125e7896624, 634 (Float[1,64,64,64]) -> 637 (Float[1,256,64,64])\n",
    "Layer(CudnnConvolution): Conv_5 + Relu_6, Tactic: 0x0000000000000038, 637 (Float[1,256,64,64]) -> 640 (Float[1,256,64,64])\n",
    "Layer(CaskConvolution): Conv_8, Tactic: 0x5e7d1125e7896624, 634 (Float[1,64,64,64]) -> 990 (Float[1,256,64,64])\n",
    "Layer(CaskConvolution): Conv_7 + Add_9 + Relu_10, Tactic: 0x7e29bdfccd92c42c, 640 (Float[1,256,64,64]), 990 (Float[1,256,64,64]) -> 646 (Float[1,256,64,64])\n",
    "Layer(CaskConvolution): Conv_11 + Relu_12, Tactic: 0x90238daf8750ddb0, 646 (Float[1,256,64,64]) -> 649 (Float[1,256,64,64])\n",
    "Layer(CudnnConvolution): Conv_13 + Relu_14, Tactic: 0x0000000000000038, 649 (Float[1,256,64,64]) -> 652 (Float[1,256,64,64])\n",
    "Layer(CaskConvolution): Conv_15 + Add_16 + Relu_17, Tactic: 0x7e29bdfccd92c42c, 652 (Float[1,256,64,64]), 646 (Float[1,256,64,64]) -> 656 (Float[1,256,64,64])\n",
    "Layer(CaskConvolution): Conv_18 + Relu_19, Tactic: 0x90238daf8750ddb0, 656 (Float[1,256,64,64]) -> 659 (Float[1,256,64,64])\n",
    "Layer(CudnnConvolution): Conv_20 + Relu_21, Tactic: 0x0000000000000038, 659 (Float[1,256,64,64]) -> 662 (Float[1,256,64,64])\n",
    "Layer(CaskConvolution): Conv_22 + Add_23 + Relu_24, Tactic: 0x7e29bdfccd92c42c, 662 (Float[1,256,64,64]), 656 (Float[1,256,64,64]) -> Reformatted Output Tensor 0 to Conv_22 + Add_23 + Relu_24 (Float[1,256,64,64])\n",
    "Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_22 + Add_23 + Relu_24, Tactic: 0x00000000000003ea, Reformatted Output Tensor 0 to Conv_22 + Add_23 + Relu_24 (Float[1,256,64,64]) -> 666 (Float[1,256:4,64,64])\n",
    "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to Conv_25 + Relu_26, Tactic: 0x0000000000000000, 666 (Float[1,256:4,64,64]) -> Reformatted Input Tensor 0 to Conv_25 + Relu_26 (Float[1,256,64,64])\n",
    "Layer(CaskConvolution): Conv_25 + Relu_26, Tactic: 0xf92663d88255134b, Reformatted Input Tensor 0 to Conv_25 + Relu_26 (Float[1,256,64,64]) -> 669 (Float[1,512,64,64])\n",
    "Layer(CaskConvolution): Conv_27 + Relu_28, Tactic: 0x19799dabb3912d88, 669 (Float[1,512,64,64]) -> 672 (Float[1,512,32,32])\n",
    "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to Conv_30, Tactic: 0x0000000000000000, 666 (Float[1,256:4,64,64]) -> Reformatted Input Tensor 0 to Conv_30 (Float[1,256,64,64])\n",
    "Layer(CaskConvolution): Conv_30, Tactic: 0x852b455de4263ff7, Reformatted Input Tensor 0 to Conv_30 (Float[1,256,64,64]) -> 1020 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_29 + Add_31 + Relu_32, Tactic: 0xf92663d88255134b, 672 (Float[1,512,32,32]), 1020 (Float[1,512,32,32]) -> 678 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_33 + Relu_34, Tactic: 0xf92663d88255134b, 678 (Float[1,512,32,32]) -> 681 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_35 + Relu_36, Tactic: 0x19799dabb3912d88, 681 (Float[1,512,32,32]) -> 684 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_37 + Add_38 + Relu_39, Tactic: 0xf92663d88255134b, 684 (Float[1,512,32,32]), 678 (Float[1,512,32,32]) -> 688 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_40 + Relu_41, Tactic: 0xf92663d88255134b, 688 (Float[1,512,32,32]) -> 691 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_42 + Relu_43, Tactic: 0x19799dabb3912d88, 691 (Float[1,512,32,32]) -> 694 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_44 + Add_45 + Relu_46, Tactic: 0xf92663d88255134b, 694 (Float[1,512,32,32]), 688 (Float[1,512,32,32]) -> 698 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_47 + Relu_48, Tactic: 0xf92663d88255134b, 698 (Float[1,512,32,32]) -> 701 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_49 + Relu_50, Tactic: 0x19799dabb3912d88, 701 (Float[1,512,32,32]) -> 704 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_51 + Add_52 + Relu_53, Tactic: 0xf92663d88255134b, 704 (Float[1,512,32,32]), 698 (Float[1,512,32,32]) -> 708 (Float[1,512,32,32])\n",
    "Layer(CaskConvolution): Conv_54 + Relu_55, Tactic: 0x852b455de4263ff7, 708 (Float[1,512,32,32]) -> 711 (Float[1,1024,32,32])\n",
    "Layer(CaskConvolution): Conv_56 + Relu_57, Tactic: 0xfa1e150a2da08264, 711 (Float[1,1024,32,32]) -> 714 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_59, Tactic: 0xfa1e150a2da08265, 708 (Float[1,512,32,32]) -> 1059 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_58 + Add_60 + Relu_61, Tactic: 0x90d45931b538d74f, 714 (Float[1,1024,16,16]), 1059 (Float[1,1024,16,16]) -> 720 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_62 + Relu_63, Tactic: 0x90d45931b538d74f, 720 (Float[1,1024,16,16]) -> 723 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_64 + Relu_65, Tactic: 0x516049bee7812ab1, 723 (Float[1,1024,16,16]) -> 726 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_66 + Add_67 + Relu_68, Tactic: 0x90d45931b538d74f, 726 (Float[1,1024,16,16]), 720 (Float[1,1024,16,16]) -> 730 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_69 + Relu_70, Tactic: 0x90d45931b538d74f, 730 (Float[1,1024,16,16]) -> 733 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_71 + Relu_72, Tactic: 0x516049bee7812ab1, 733 (Float[1,1024,16,16]) -> 736 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_73 + Add_74 + Relu_75, Tactic: 0x90d45931b538d74f, 736 (Float[1,1024,16,16]), 730 (Float[1,1024,16,16]) -> 740 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_76 + Relu_77, Tactic: 0x90d45931b538d74f, 740 (Float[1,1024,16,16]) -> 743 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_78 + Relu_79, Tactic: 0x516049bee7812ab1, 743 (Float[1,1024,16,16]) -> 746 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_80 + Add_81 + Relu_82, Tactic: 0x90d45931b538d74f, 746 (Float[1,1024,16,16]), 740 (Float[1,1024,16,16]) -> 750 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_83 + Relu_84, Tactic: 0x90d45931b538d74f, 750 (Float[1,1024,16,16]) -> 753 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_85 + Relu_86, Tactic: 0x516049bee7812ab1, 753 (Float[1,1024,16,16]) -> 756 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_87 + Add_88 + Relu_89, Tactic: 0x90d45931b538d74f, 756 (Float[1,1024,16,16]), 750 (Float[1,1024,16,16]) -> 760 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_90 + Relu_91, Tactic: 0x90d45931b538d74f, 760 (Float[1,1024,16,16]) -> 763 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_92 + Relu_93, Tactic: 0x516049bee7812ab1, 763 (Float[1,1024,16,16]) -> 766 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_94 + Add_95 + Relu_96, Tactic: 0x90d45931b538d74f, 766 (Float[1,1024,16,16]), 760 (Float[1,1024,16,16]) -> 770 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_97 + Relu_98, Tactic: 0x90d45931b538d74f, 770 (Float[1,1024,16,16]) -> 773 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_99 + Relu_100, Tactic: 0x516049bee7812ab1, 773 (Float[1,1024,16,16]) -> 776 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_101 + Add_102 + Relu_103, Tactic: 0x90d45931b538d74f, 776 (Float[1,1024,16,16]), 770 (Float[1,1024,16,16]) -> 780 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_104 + Relu_105, Tactic: 0x90d45931b538d74f, 780 (Float[1,1024,16,16]) -> 783 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_106 + Relu_107, Tactic: 0x516049bee7812ab1, 783 (Float[1,1024,16,16]) -> 786 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_108 + Add_109 + Relu_110, Tactic: 0x90d45931b538d74f, 786 (Float[1,1024,16,16]), 780 (Float[1,1024,16,16]) -> 790 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_111 + Relu_112, Tactic: 0x90d45931b538d74f, 790 (Float[1,1024,16,16]) -> 793 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_113 + Relu_114, Tactic: 0x516049bee7812ab1, 793 (Float[1,1024,16,16]) -> 796 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_115 + Add_116 + Relu_117, Tactic: 0x90d45931b538d74f, 796 (Float[1,1024,16,16]), 790 (Float[1,1024,16,16]) -> 800 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_118 + Relu_119, Tactic: 0x90d45931b538d74f, 800 (Float[1,1024,16,16]) -> 803 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_120 + Relu_121, Tactic: 0x516049bee7812ab1, 803 (Float[1,1024,16,16]) -> 806 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_122 + Add_123 + Relu_124, Tactic: 0x90d45931b538d74f, 806 (Float[1,1024,16,16]), 800 (Float[1,1024,16,16]) -> 810 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_125 + Relu_126, Tactic: 0x90d45931b538d74f, 810 (Float[1,1024,16,16]) -> 813 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_127 + Relu_128, Tactic: 0x516049bee7812ab1, 813 (Float[1,1024,16,16]) -> 816 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_129 + Add_130 + Relu_131, Tactic: 0x90d45931b538d74f, 816 (Float[1,1024,16,16]), 810 (Float[1,1024,16,16]) -> 820 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_132 + Relu_133, Tactic: 0x90d45931b538d74f, 820 (Float[1,1024,16,16]) -> 823 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_134 + Relu_135, Tactic: 0x516049bee7812ab1, 823 (Float[1,1024,16,16]) -> 826 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_136 + Add_137 + Relu_138, Tactic: 0x90d45931b538d74f, 826 (Float[1,1024,16,16]), 820 (Float[1,1024,16,16]) -> 830 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_139 + Relu_140, Tactic: 0x90d45931b538d74f, 830 (Float[1,1024,16,16]) -> 833 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_141 + Relu_142, Tactic: 0x516049bee7812ab1, 833 (Float[1,1024,16,16]) -> 836 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_143 + Add_144 + Relu_145, Tactic: 0x90d45931b538d74f, 836 (Float[1,1024,16,16]), 830 (Float[1,1024,16,16]) -> 840 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_146 + Relu_147, Tactic: 0x90d45931b538d74f, 840 (Float[1,1024,16,16]) -> 843 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_148 + Relu_149, Tactic: 0x516049bee7812ab1, 843 (Float[1,1024,16,16]) -> 846 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_150 + Add_151 + Relu_152, Tactic: 0x90d45931b538d74f, 846 (Float[1,1024,16,16]), 840 (Float[1,1024,16,16]) -> 850 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_153 + Relu_154, Tactic: 0x90d45931b538d74f, 850 (Float[1,1024,16,16]) -> 853 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_155 + Relu_156, Tactic: 0x516049bee7812ab1, 853 (Float[1,1024,16,16]) -> 856 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_157 + Add_158 + Relu_159, Tactic: 0x90d45931b538d74f, 856 (Float[1,1024,16,16]), 850 (Float[1,1024,16,16]) -> 860 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_160 + Relu_161, Tactic: 0x90d45931b538d74f, 860 (Float[1,1024,16,16]) -> 863 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_162 + Relu_163, Tactic: 0x516049bee7812ab1, 863 (Float[1,1024,16,16]) -> 866 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_164 + Add_165 + Relu_166, Tactic: 0x90d45931b538d74f, 866 (Float[1,1024,16,16]), 860 (Float[1,1024,16,16]) -> 870 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_167 + Relu_168, Tactic: 0x90d45931b538d74f, 870 (Float[1,1024,16,16]) -> 873 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_169 + Relu_170, Tactic: 0x516049bee7812ab1, 873 (Float[1,1024,16,16]) -> 876 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_171 + Add_172 + Relu_173, Tactic: 0x90d45931b538d74f, 876 (Float[1,1024,16,16]), 870 (Float[1,1024,16,16]) -> 880 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_174 + Relu_175, Tactic: 0x90d45931b538d74f, 880 (Float[1,1024,16,16]) -> 883 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_176 + Relu_177, Tactic: 0x516049bee7812ab1, 883 (Float[1,1024,16,16]) -> 886 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_178 + Add_179 + Relu_180, Tactic: 0x90d45931b538d74f, 886 (Float[1,1024,16,16]), 880 (Float[1,1024,16,16]) -> 890 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_181 + Relu_182, Tactic: 0x90d45931b538d74f, 890 (Float[1,1024,16,16]) -> 893 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_183 + Relu_184, Tactic: 0x516049bee7812ab1, 893 (Float[1,1024,16,16]) -> 896 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_185 + Add_186 + Relu_187, Tactic: 0x90d45931b538d74f, 896 (Float[1,1024,16,16]), 890 (Float[1,1024,16,16]) -> 900 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_188 + Relu_189, Tactic: 0x90d45931b538d74f, 900 (Float[1,1024,16,16]) -> 903 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_190 + Relu_191, Tactic: 0x516049bee7812ab1, 903 (Float[1,1024,16,16]) -> 906 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_192 + Add_193 + Relu_194, Tactic: 0x90d45931b538d74f, 906 (Float[1,1024,16,16]), 900 (Float[1,1024,16,16]) -> 910 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_195 + Relu_196, Tactic: 0x90d45931b538d74f, 910 (Float[1,1024,16,16]) -> 913 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_197 + Relu_198, Tactic: 0x516049bee7812ab1, 913 (Float[1,1024,16,16]) -> 916 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_199 + Add_200 + Relu_201, Tactic: 0x90d45931b538d74f, 916 (Float[1,1024,16,16]), 910 (Float[1,1024,16,16]) -> 920 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_202 + Relu_203, Tactic: 0x90d45931b538d74f, 920 (Float[1,1024,16,16]) -> 923 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_204 + Relu_205, Tactic: 0x516049bee7812ab1, 923 (Float[1,1024,16,16]) -> 926 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_206 + Add_207 + Relu_208, Tactic: 0x90d45931b538d74f, 926 (Float[1,1024,16,16]), 920 (Float[1,1024,16,16]) -> 930 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_209 + Relu_210, Tactic: 0x90d45931b538d74f, 930 (Float[1,1024,16,16]) -> 933 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_211 + Relu_212, Tactic: 0x516049bee7812ab1, 933 (Float[1,1024,16,16]) -> 936 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_213 + Add_214 + Relu_215, Tactic: 0x90d45931b538d74f, 936 (Float[1,1024,16,16]), 930 (Float[1,1024,16,16]) -> 940 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_216 + Relu_217, Tactic: 0x90d45931b538d74f, 940 (Float[1,1024,16,16]) -> 943 (Float[1,2048,16,16])\n",
    "Layer(CaskConvolution): Conv_218 + Relu_219, Tactic: 0xfa1e150a2da08264, 943 (Float[1,2048,16,16]) -> 946 (Float[1,2048,8,8])\n",
    "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_221, Tactic: 0x0000000000000000, 940 (Float[1,1024,16,16]) -> Reformatted Input Tensor 0 to Conv_221 (Float[1,1024,16,16])\n",
    "Layer(CaskConvolution): Conv_221, Tactic: 0xe40b38338a3a7d7e, Reformatted Input Tensor 0 to Conv_221 (Float[1,1024,16,16]) -> 1269 (Float[1,2048,8,8])\n",
    "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_220 + Add_222 + Relu_223, Tactic: 0x00000000000003ea, 946 (Float[1,2048,8,8]) -> Reformatted Input Tensor 0 to Conv_220 + Add_222 + Relu_223 (Float[1,2048,8,8])\n",
    "Layer(CudnnConvolution): Conv_220 + Add_222 + Relu_223, Tactic: 0x0000000000000039, Reformatted Input Tensor 0 to Conv_220 + Add_222 + Relu_223 (Float[1,2048,8,8]), 1269 (Float[1,2048,8,8]) -> 952 (Float[1,2048,8,8])\n",
    "Layer(CudnnConvolution): Conv_224 + Relu_225, Tactic: 0x0000000000000039, 952 (Float[1,2048,8,8]) -> 955 (Float[1,2048,8,8])\n",
    "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_226 + Relu_227, Tactic: 0x00000000000003ea, 955 (Float[1,2048,8,8]) -> Reformatted Input Tensor 0 to Conv_226 + Relu_227 (Float[1,2048,8,8])\n",
    "Layer(CaskConvolution): Conv_226 + Relu_227, Tactic: 0xb132670a7750e064, Reformatted Input Tensor 0 to Conv_226 + Relu_227 (Float[1,2048,8,8]) -> 958 (Float[1,2048,8,8])\n",
    "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_228 + Add_229 + Relu_230, Tactic: 0x00000000000003ea, 958 (Float[1,2048,8,8]) -> Reformatted Input Tensor 0 to Conv_228 + Add_229 + Relu_230 (Float[1,2048,8,8])\n",
    "Layer(CudnnConvolution): Conv_228 + Add_229 + Relu_230, Tactic: 0x0000000000000039, Reformatted Input Tensor 0 to Conv_228 + Add_229 + Relu_230 (Float[1,2048,8,8]), 952 (Float[1,2048,8,8]) -> 962 (Float[1,2048,8,8])\n",
    "Layer(CudnnConvolution): Conv_231 + Relu_232, Tactic: 0x0000000000000039, 962 (Float[1,2048,8,8]) -> 965 (Float[1,2048,8,8])\n",
    "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_233 + Relu_234, Tactic: 0x00000000000003ea, 965 (Float[1,2048,8,8]) -> Reformatted Input Tensor 0 to Conv_233 + Relu_234 (Float[1,2048,8,8])\n",
    "Layer(CaskConvolution): Conv_233 + Relu_234, Tactic: 0xb132670a7750e064, Reformatted Input Tensor 0 to Conv_233 + Relu_234 (Float[1,2048,8,8]) -> 968 (Float[1,2048,8,8])\n",
    "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_235 + Add_236 + Relu_237, Tactic: 0x00000000000003ea, 968 (Float[1,2048,8,8]) -> Reformatted Input Tensor 0 to Conv_235 + Add_236 + Relu_237 (Float[1,2048,8,8])\n",
    "Layer(CudnnConvolution): Conv_235 + Add_236 + Relu_237, Tactic: 0x0000000000000039, Reformatted Input Tensor 0 to Conv_235 + Add_236 + Relu_237 (Float[1,2048,8,8]), 962 (Float[1,2048,8,8]) -> 972 (Float[1,2048,8,8])\n",
    "Layer(CaskPooling): GlobalAveragePool_238, Tactic: 0x933eceba7b866d59, 972 (Float[1,2048,8,8]) -> 973 (Float[1,2048,1,1])\n",
    "Layer(CaskGemmConvolution): Gemm_240, Tactic: 0x00000000000200ba, 973 (Float[1,2048,1,1]) -> Gemm_240_out_tensor (Float[1,128,1,1])\n",
    "Layer(NoOp): reshape_after_Gemm_240, Tactic: 0x0000000000000000, Gemm_240_out_tensor (Float[1,128,1,1]) -> features (Float[1,128])\n",
    "Layer(Shuffle): reshape_before_Gemm_241, Tactic: 0x0000000000000000, features (Float[1,128]) -> reshape_before_Gemm_241_out_tensor (Float[1,128,1,1])\n",
    "Layer(CaskGemmConvolution): Gemm_241, Tactic: 0x000000000002015b, reshape_before_Gemm_241_out_tensor (Float[1,128,1,1]) -> Gemm_241_out_tensor (Float[1,1,1,1])\n",
    "Layer(NoOp): reshape_after_Gemm_241, Tactic: 0x0000000000000000, Gemm_241_out_tensor (Float[1,1,1,1]) -> 976 (Float[1,1])\n",
    "Layer(PointWiseV2): PWN(Sigmoid_242), Tactic: 0x0000000000000000, 976 (Float[1,1]) -> score (Float[1,1])\n",
    "`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里的结果类似于torch/tf自带的可视化，这个也是。看起来有点费劲，不过好在还有其他可视化的方法。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 02 trex\n",
    "原始文件来源于[github](https://github.com/NVIDIA/TensorRT/blob/main/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb)\n",
    "\n",
    "`trtexec --explicitBatch --onnx=model/classify.onnx --saveEngine=model/classify_fp32.trt --dumpProfile --exportProfile=model/profile_json.json --exportLayerInfo=model/graph_json.json --profil\n",
    "ingVerbosity=detailed\n",
    "`\n",
    "可以导出`json`文件\n",
    "\n",
    "接下来的步骤，需要安装trex，trex来自于TensorRT，需下载后按步骤安装(这里lz4是一个坑，安装了3.0.0）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trex'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-483573d2408e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mmodule_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabspath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'.'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtrex\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;31m# Configure a wider output (for the wide graphs)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'trex'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "from trex import *\n",
    "\n",
    "# Configure a wider output (for the wide graphs)\n",
    "set_wide_display()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EnginePlan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-e6c7e922d371>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mplan\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mEnginePlan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'model/graph_json.json'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'model/profile_json.json'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Summary for {plan.name}:\\n\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mplan\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mgraph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mto_dot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mplan\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlayer_type_formatter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'EnginePlan' is not defined"
     ]
    }
   ],
   "source": [
    "plan = EnginePlan('model/graph_json.json', 'model/profile_json.json')\n",
    "print(f\"Summary for {plan.name}:\\n\")\n",
    "plan.summary()\n",
    "\n",
    "graph = to_dot(plan, layer_type_formatter)\n",
    "svg_name = render_dot(graph,'model/classify', 'svg')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
